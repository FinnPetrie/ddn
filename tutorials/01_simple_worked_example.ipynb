{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constrained Declarative Node Worked Examples\n",
    "\n",
    "In this notebook we explore three simple examples of constrained declarative nodes.\n",
    "The first example solves a problem with linear objective and unit sphere constraint,\n",
    "the second example solves a problem with quadratic objective and unit sphere constraint,\n",
    "and the third example solve a problem with quadratic objective and unit ball constraint.\n",
    "All examples have two-dimensional output to allow for easy visualization.\n",
    "\n",
    "It is assumed that you have read the [\"Deep Declarative Networks: A New Hope\"](https://arxiv.org/abs/1909.04866) paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "from autograd import jacobian\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Minimize linear objective over unit circle\n",
    "\n",
    "Consider the problem\n",
    "$$\n",
    "\\begin{array}{rll}\n",
    "y(x) =& \\text{argmin}_u & (1, x)^T u\n",
    "\\\\\n",
    "& \\text{subject to} & \\|u\\|_2^2 = 1\n",
    "\\end{array}\n",
    "$$\n",
    "where $x \\in \\mathbb{R}$ and $y(x) \\in \\mathbb{R}^2$.\n",
    "\n",
    "This problem has an analytic solution\n",
    "$$\n",
    "\\begin{align*}\n",
    "y &= \\frac{-1}{\\sqrt{1 + x^2}} \\begin{bmatrix} 1 \\\\ x \\end{bmatrix} \\\\\n",
    "\\text{D}y &= \\begin{bmatrix} \\frac{\\text{d}y_1}{\\text{d}x} \\\\ \\frac{\\text{d}y_2}{\\text{d}x} \\end{bmatrix}\n",
    "= \\frac{1}{(1 + x^2)^{3/2}} \\begin{bmatrix} x \\\\ -1 \\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "which we have implemented in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# objective and constraint functions\n",
    "\n",
    "def f(x, u):\n",
    "    \"\"\"Objective function taking x \\in \\reals, u \\in \\reals^2.\"\"\"\n",
    "    return np.dot(u, np.array([1.0, x]))\n",
    "\n",
    "def h(u):\n",
    "    \"\"\"Constraint function taking u \\in \\reals^2.\"\"\"\n",
    "    return np.dot(u, u) - 1.0\n",
    "\n",
    "# analytical solutions\n",
    "\n",
    "def solve(x):\n",
    "    \"\"\"Analytical solution to min. f s.t. h = 0. Returns both optimal primal and dual variables.\"\"\"\n",
    "    return -1.0 / np.sqrt(1.0 + x**2.0) * np.array([1.0, x]), -0.5 * np.sqrt(1 + x**2.0)\n",
    "\n",
    "def dy_star(x):\n",
    "    \"\"\"Analytical derivative of y with respect to x.\"\"\"\n",
    "    return 1.0 / np.power(1 + x**2.0, 1.5) * np.array([x, -1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the gradient using implicit differentiation on the first-order optimality condition of the Lagrangian,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "0 &= \n",
    "\\text{D} \\begin{bmatrix}\n",
    "    \\text{D}_{Y} {\\cal L}(x, y, \\lambda) \\\\\n",
    "    \\text{D}_{\\Lambda} {\\cal L}(x, y, \\lambda)\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "&=\n",
    "\\text{D} \\begin{bmatrix}\n",
    "    \\text{D}_Y f(x, y) - \\lambda \\text{D}_Y h(y) \\\\\n",
    "    h(y)\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "    \\text{D}^2_{XY} f(x, y) + \\text{D}^2_{YY} f(x, y) \\text{D}y - \\lambda \\text{D}^2_{YY} h(y) \\text{D}y - (\\text{D}_Y h(y))^T \\text{D}\\lambda \\\\\n",
    "    \\text{D}_{Y} h(y) \\text{D}y\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "    \\text{D}^2_{XY} f(x, y) \\\\\n",
    "    0\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "    \\text{D}^2_{YY} f(x, y) - \\lambda \\text{D}^2_{YY} h(y) & -(\\text{D}_Y h(y))^T \\\\\n",
    "    \\text{D}_{Y} h(y) & 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    \\text{D}y \\\\\n",
    "    \\text{D}\\lambda\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "    b \\\\\n",
    "    0\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "    H & -a \\\\\n",
    "    a^T & 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    \\text{D}y \\\\\n",
    "    \\text{D}\\lambda\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Block elimination then gives\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{D} y\n",
    "&= \\left( \\frac{H^{-1}aa^TH^{-1}}{a^T H^{-1} a} - H^{-1}\\right) b \\\\\n",
    "&= \\frac{v^T b}{v^T a} v - w\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $v = H^{-1}a$ and $w = H^{-1}b$, and which we have implemented below using automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gradient by implicit differentiation\n",
    "\n",
    "fY = grad(f, 1)\n",
    "hY = grad(h)\n",
    "fXY = jacobian(fY, 0)\n",
    "fYY = jacobian(fY, 1)\n",
    "hYY = jacobian(hY, 0)\n",
    "\n",
    "def dy(x):\n",
    "    \"\"\"Compute gradient of y with respect to x using implicit differentiation.\"\"\"\n",
    "\n",
    "    y, nu = solve(x)\n",
    "\n",
    "    # Here we solve a system of linear equations rather than inverting H. The linear\n",
    "    # algebra solver gives $w = H^{-1} D_Y h$ and $v = H^{-1} D^2_{XY} f$ from which\n",
    "    # we compute Dy(x) as (w^T D^2_{XY} f / w^T D_{Y} h) w - v. \n",
    "\n",
    "    a = hY(y)\n",
    "    b = fXY(x, y)\n",
    "    H = fYY(x, y) - nu * hYY(y)\n",
    "    \n",
    "    (v, w) = np.linalg.solve(H, np.stack((a, b)))\n",
    "    return v.dot(b) / v.dot(a) * v - w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following compares the analytic gradient against the gradient derived implicitly over the range $x \\in [-2, 2]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max. difference between implicit and analytic gradients is 1.1102230246251565e-16\n"
     ]
    }
   ],
   "source": [
    "# generate data for different input\n",
    "x = np.linspace(-2.0, 2.0, num=51)\n",
    "y = [solve(xi)[0] for xi in x]\n",
    "dy_analytic = [dy_star(xi) for xi in x]\n",
    "dy_implicit = [dy(xi) for xi in x]\n",
    "\n",
    "# print difference between analytic and implicit gradients\n",
    "err = np.abs(np.array(dy_analytic) - np.array(dy_implicit))\n",
    "print(\"Max. difference between implicit and analytic gradients is {}\".format(np.max(err)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animation\n",
    "\n",
    "To get a better understanding of how the output of this constrained declarative node changes with input---a requirement for end-to-end learning---we animate the geometry of the solution to the problem $y$ as we vary the input $x$. Along with the animation we plot both $y$ and $\\text{D}y$ as functions of $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# simple animation of the problem\n",
    "\n",
    "def init():\n",
    "    for a in ax:\n",
    "        a.axis('square')\n",
    "        a.set_xlim(x[0], x[-1])    \n",
    "    ax[0].set_ylim(x[0], x[-1])\n",
    "    ax[1].set_ylim(-1.0, 1.0)\n",
    "    ax[2].set_ylim(-1.0, 1.0)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def animate(fnum, x, y, dy):\n",
    "    \n",
    "    for a in ax:\n",
    "        a.clear()\n",
    "        \n",
    "    ax[0].annotate(\"\", xy=(1.0, x[fnum]), xytext=(0, 0), arrowprops = dict(arrowstyle=\"->\", linewidth=3, color='b'))\n",
    "    for delta in np.linspace(-2.0, 2.0, 5):\n",
    "        dx = delta * np.sqrt(1.0 + x[fnum]**2.0)\n",
    "        ax[0].plot([-2.0 * x[fnum] + dx, 2.0 * x[fnum] + dx], [2.0, -2.0], 'b--', linewidth=1)\n",
    "\n",
    "    ax[0].plot(np.cos(np.linspace(0.0, 2.0 * np.pi)), np.sin(np.linspace(0.0, 2.0 * np.pi)), 'r--', linewidth=1)\n",
    "    ax[0].plot(y[fnum][0], y[fnum][1], 'ro', markersize=12, linewidth=2)\n",
    "    ax[0].set_xlabel(r\"$u_1$\"); ax[0].set_ylabel(r\"$u_2$\")\n",
    "\n",
    "    ax[1].plot(x[0:fnum], [yi[0] for yi in y[0:fnum]], x[0:fnum], [yi[1] for yi in y[0:fnum]])\n",
    "    ax[1].legend([r\"$y_1$\", r\"$y_2$\"])\n",
    "    \n",
    "    ax[2].plot(x[0:fnum], [di[0] for di in dy[0:fnum]], x[0:fnum], [di[1] for di in dy[0:fnum]])\n",
    "    ax[2].legend([r\"$Dy_1$\", r\"$Dy_2$\"], loc='upper left')\n",
    "    \n",
    "    for a in ax:\n",
    "        a.axis('square')\n",
    "        a.set_xlim(x[0], x[-1])\n",
    "    ax[0].set_ylim(x[0], x[-1])\n",
    "    ax[1].set_ylim(-1.0, 1.0)\n",
    "    ax[2].set_ylim(-1.0, 1.0)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = [plt.subplot(1, 2, 1), plt.subplot(2, 2, 2), plt.subplot(2, 2, 4)]\n",
    "plt.suptitle(r\"$y$ = argmin $(1, x)^T u$ subject to $\\|u\\|^2 = 1$\")\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, init_func=init, fargs=(x, y, dy_implicit),\n",
    "                              interval=100, frames=len(x), blit=False, repeat=False)\n",
    "\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# display using video or javascript\n",
    "\n",
    "HTML(ani.to_html5_video())\n",
    "#HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Minimize quadratic objective over unit circle\n",
    "\n",
    "Consider the problem\n",
    "$$\n",
    "\\begin{array}{rll}\n",
    "y(x) =& \\text{argmin}_u & \\frac{1}{2} u^T u - x^T u\n",
    "\\\\\n",
    "& \\text{subject to} & \\|u\\|_2^2 = 1\n",
    "\\end{array}\n",
    "$$\n",
    "with $x \\in \\mathbb{R}^2$ and $y \\in \\mathbb{R}^2$.\n",
    "\n",
    "This is just the Euclidean projection of the two-dimensional point $x$ onto the unit cirle, which can be seen by replacing the objective function with $\\frac{1}{2} \\|u - x\\|^2 = \\frac{1}{2} u^T u - x^T u + \\frac{1}{2} x^T x$ and recognizing that $\\frac{1}{2} x^T x$ plays no part in the optimization. The problem has analytic solution\n",
    "$$\n",
    "\\begin{align*}\n",
    "y &= \\frac{1}{\\|x\\|} x \\\\\n",
    "\\text{D} y &= \\begin{bmatrix}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2} \\\\\n",
    "\\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2}\n",
    "\\end{bmatrix}\n",
    "= \\frac{1}{\\|x\\|^3} \\begin{bmatrix}\n",
    "    x_2^2 & - x_1 x_2 \\\\\n",
    "    - x_1 x_2 & x_1^2\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# objective and constraint functions\n",
    "\n",
    "def f(x, u):\n",
    "    \"\"\"Objective function taking x \\in \\reals^2, u \\in \\reals^2.\"\"\"\n",
    "    return 0.5 * np.dot(u, u) - np.dot(u, x)\n",
    "\n",
    "def h(u):\n",
    "    \"\"\"Constraint function taking u \\in \\reals^2.\"\"\"\n",
    "    return np.dot(u, u) - 1.0\n",
    "\n",
    "# analytical solutions\n",
    "\n",
    "def solve(x):\n",
    "    \"\"\"Analytical solution to min. f s.t. h = 0. Returns both optimal primal and dual variables.\"\"\"\n",
    "    return 1.0 / np.sqrt(np.dot(x, x)) * x, None\n",
    "\n",
    "def dy_star(x):\n",
    "    \"\"\"Analytical derivative of y with respect to x.\"\"\"\n",
    "    return 1.0 / np.power(np.dot(x, x), 1.5) * np.array([[x[1] ** 2, -x[0]*x[1]], [-x[0]*x[1], x[0] ** 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can also compute the gradient using implicit differentiation. Now we have two-dimensional inputs and outputs. We also assume that our solver does not return $\\lambda$ and so have to calculate it in the gradient function as\n",
    "\n",
    "$$\n",
    "\\lambda = \\frac{\\text{D}_Y f(x, y)_i}{\\text{D}_Y h(y)_i}\n",
    "$$\n",
    "for any $i$ such that $\\text{D}_Y h(y)_i \\neq 0.$\n",
    "\n",
    "The gradient is\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{D} y\n",
    "&= \\left( \\frac{H^{-1}aa^TH^{-1}}{a^T H^{-1} a} - H^{-1}\\right) B \\\\\n",
    "&= \\frac{1}{v^T a} v v^T B - W\n",
    "\\end{align*}\n",
    "$$\n",
    "where $v = H^{-1}a$ and $W = H^{-1}B$, and which we have implemented below using automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gradient by implicit differentiation\n",
    "\n",
    "fY = grad(f, 1)\n",
    "hY = grad(h)\n",
    "fXY = jacobian(fY, 0)\n",
    "fYY = jacobian(fY, 1)\n",
    "hYY = jacobian(hY, 0)\n",
    "\n",
    "def dy(x):\n",
    "    \"\"\"Compute gradient of y with respect to x using implicit differentiation.\"\"\"\n",
    "\n",
    "    y, nu = solve(x)\n",
    "    \n",
    "    a = hY(y)\n",
    "    b = fXY(x, y)\n",
    "    if (nu is None):\n",
    "        indx = np.nonzero(a)[0]\n",
    "        nu = 0 if (len(indx) == 0) else fY(x, y)[indx[0]] / a[indx[0]] \n",
    "\n",
    "    H = fYY(x, y) - nu * hYY(y)\n",
    "    \n",
    "    # Here we solve a system of linear equations rather than inverting H.\n",
    "    w = np.linalg.solve(H, np.concatenate((a.reshape((len(y), 1)), b), axis=1))\n",
    "    return (np.outer(w[:, 0], w[:, 0].dot(b) / w[:, 0].dot(a)) - w[:, 1:len(x) + 1]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max. difference between implicit and analytic gradients is 4.996003610813204e-16\n"
     ]
    }
   ],
   "source": [
    "# generate data for different input\n",
    "x = np.linspace(-2.0, 2.0, num=51)\n",
    "y = [solve(np.array([0.5, xi]))[0] for xi in x]\n",
    "dy_analytic = [dy_star(np.array([0.5, xi])) for xi in x]\n",
    "dy_implicit = [dy(np.array([0.5, xi])) for xi in x]\n",
    "\n",
    "# print difference between analytic and implicit gradients\n",
    "err = np.abs(np.array(dy_analytic) - np.array(dy_implicit))\n",
    "print(\"Max. difference between implicit and analytic gradients is {}\".format(np.max(err)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# simple animation of the problem\n",
    "\n",
    "def init():\n",
    "    for a in ax:\n",
    "        a.axis('square')\n",
    "        a.set_xlim(x[0], x[-1])    \n",
    "    ax[0].set_ylim(x[0], x[-1])\n",
    "    ax[1].set_ylim(-1.0, 1.0)\n",
    "    ax[2].set_ylim(-1.0, 1.0)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def animate(fnum, x, y, dy):\n",
    "    \n",
    "    for a in ax:\n",
    "        a.clear()\n",
    "\n",
    "    ax[0].annotate(\"\", xy=(-0.5, -x[fnum]), xytext=(0, 0), arrowprops=dict(arrowstyle=\"->\", linewidth=3, color='b'))\n",
    "    for r in [0.125, 0.5, 1.125, 2.0, 3.125]:\n",
    "        ax[0].plot(r * np.cos(np.linspace(0.0, 2.0 * np.pi)) + 0.5, r * np.sin(np.linspace(0.0, 2.0 * np.pi)) + x[fnum], 'b--', linewidth=1)\n",
    "\n",
    "    ax[0].plot(np.cos(np.linspace(0.0, 2.0 * np.pi)), np.sin(np.linspace(0.0, 2.0 * np.pi)), 'r--', linewidth=1)\n",
    "    ax[0].plot(y[fnum][0], y[fnum][1], 'ro', markersize=12, linewidth=2)        \n",
    "    ax[0].set_xlabel(r\"$u_1$\"); ax[0].set_ylabel(r\"$u_2$\")\n",
    "\n",
    "    ax[1].plot(x[0:fnum], [yi[0] for yi in y[0:fnum]], x[0:fnum], [yi[1] for yi in y[0:fnum]])\n",
    "    ax[1].legend([r\"$y_1$\", r\"$y_2$\"])\n",
    "    \n",
    "    ax[2].plot(x[0:fnum], [di[0, 0] for di in dy[0:fnum]], x[0:fnum], [di[0, 1] for di in dy[0:fnum]],\n",
    "               x[0:fnum], [di[1, 0] for di in dy[0:fnum]], x[0:fnum], [di[1, 1] for di in dy[0:fnum]])\n",
    "    ax[2].legend([r\"$Dy_{1,1}$\", r\"$Dy_{1,2}$\", r\"$Dy_{2,1}$\", r\"$Dy_{2,2}$\"], loc='upper left')\n",
    "    ax[2].set_xlabel(r\"$x_2$\");\n",
    "    \n",
    "    for a in ax:\n",
    "        a.axis('square')\n",
    "        a.set_xlim(x[0], x[-1])\n",
    "    ax[0].set_ylim(x[0], x[-1])\n",
    "    ax[1].set_ylim(-1.0, 1.0)\n",
    "    ax[2].set_ylim(-1.0, 1.0)\n",
    "    \n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = [plt.subplot(1, 2, 1), plt.subplot(2, 2, 2), plt.subplot(2, 2, 4)]\n",
    "plt.suptitle(r\"$y$ = argmin $\\frac{1}{2} u^T u - x^T u$ subject to $\\|u\\|^2 = 1$\")\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, init_func=init, fargs=(x, y, dy_analytic),\n",
    "                              interval=100, frames=len(x), blit=False, repeat=False)\n",
    "\n",
    "plt.close(fig)\n",
    "\n",
    "# display using video or javascript\n",
    "HTML(ani.to_html5_video())\n",
    "#HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Minimize quadratic objective over unit ball\n",
    "\n",
    "Consider the problem\n",
    "$$\n",
    "\\begin{array}{rll}\n",
    "y(x) =& \\text{argmin}_u & \\frac{1}{2} u^T u - x^T u\n",
    "\\\\\n",
    "& \\text{subject to} & \\|u\\|_2^2 \\leq 1\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The problem has analytic solution\n",
    "$$\n",
    "\\begin{align*}\n",
    "y &= \\begin{cases}\n",
    "x & \\text{if $\\|x\\| \\leq 1$} \\\\\n",
    "\\frac{1}{\\|x\\|} x & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\\\\n",
    "\\text{D} y &= \\begin{cases}\n",
    "I & \\text{if $\\|x\\| < 1$} \\\\\n",
    "\\frac{1}{\\|x\\|^3} \\begin{bmatrix}\n",
    "    x_2^2 & - x_1 x_2\n",
    "    \\\\\n",
    "    - x_1 x_2 & x_1^2\n",
    "\\end{bmatrix}\n",
    "& \\text{if $\\|x\\| > 1$}\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# objective and constraint functions\n",
    "\n",
    "def f(x, u):\n",
    "    \"\"\"Objective function taking x \\in \\reals^2, u \\in \\reals^2.\"\"\"\n",
    "    return 0.5 * np.dot(u, u) - np.dot(u, x)\n",
    "\n",
    "def h(u):\n",
    "    \"\"\"Constraint function taking u \\in \\reals^2.\"\"\"\n",
    "    return np.dot(u, u) - 1.0\n",
    "\n",
    "# analytical solutions\n",
    "\n",
    "def solve(x):\n",
    "    \"\"\"Analytical solution to min. f s.t. h = 0. Returns both optimal primal and dual variables.\"\"\"\n",
    "    d = np.dot(x, x)\n",
    "    return x.copy() if (d <= 1.0) else 1.0 / np.sqrt(d) * x, None\n",
    "\n",
    "def dy_star(x):\n",
    "    \"\"\"Analytical derivative of y with respect to x.\"\"\"\n",
    "    d = np.dot(x, x)\n",
    "    return np.eye(2, 2) if (d < 1.0) else 1.0 / np.power(d, 1.5) * np.array([[x[1] ** 2, -x[0]*x[1]], [-x[0]*x[1], x[0] ** 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gradient by implicit differentiation\n",
    "\n",
    "fY = grad(f, 1)\n",
    "hY = grad(h)\n",
    "fXY = jacobian(fY, 0)\n",
    "fYY = jacobian(fY, 1)\n",
    "hYY = jacobian(hY, 0)\n",
    "\n",
    "def dy(x):\n",
    "    \"\"\"Compute gradient of y with respect to x using implicit differentiation.\"\"\"\n",
    "\n",
    "    y, nu = solve(x)\n",
    "    \n",
    "    # use unconstrained gradient\n",
    "    if (h(x) <= 0.0):\n",
    "        return -1.0 * np.linalg.solve(fYY(x, y), fXY(x, y))\n",
    "    \n",
    "    # use constrained gradient\n",
    "    a = hY(y)\n",
    "    b = fXY(x, y)\n",
    "    if (nu is None):\n",
    "        indx = np.nonzero(a)[0]\n",
    "        nu = 0.0 if (len(indx) == 0) else fY(x, y)[indx[0]] / a[indx[0]] \n",
    "\n",
    "    H = fYY(x, y) - nu * hYY(y)\n",
    "    \n",
    "    # Here we solve a system of linear equations rather than inverting H.\n",
    "    w = np.linalg.solve(H, np.concatenate((a.reshape((len(y), 1)), b), axis=1))\n",
    "    return (np.outer(w[:, 0], w[:, 0].dot(b) / w[:, 0].dot(a)) - w[:, 1:len(x) + 1]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max. difference between implicit and analytic gradients is 1.942890293094024e-16\n"
     ]
    }
   ],
   "source": [
    "# generate data for different input\n",
    "x = np.linspace(-2.0, 2.0, num=51)\n",
    "y = [solve(np.array([0.5, xi]))[0] for xi in x]\n",
    "dy_analytic = [dy_star(np.array([0.5, xi])) for xi in x]\n",
    "dy_implicit = [dy(np.array([0.5, xi])) for xi in x]\n",
    "\n",
    "# print difference between analytic and implicit gradients\n",
    "err = np.abs(np.array(dy_analytic) - np.array(dy_implicit))\n",
    "print(\"Max. difference between implicit and analytic gradients is {}\".format(np.max(err)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = [plt.subplot(1, 2, 1), plt.subplot(2, 2, 2), plt.subplot(2, 2, 4)]\n",
    "plt.suptitle(r\"$y$ = argmin $\\frac{1}{2} u^T u - x^T u$ subject to $\\|u\\|^2 \\leq 1$\")\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, init_func=init, fargs=(x, y, dy_analytic),\n",
    "                              interval=100, frames=len(x), blit=False, repeat=False)\n",
    "\n",
    "plt.close(fig)\n",
    "\n",
    "# display using video or javascript\n",
    "HTML(ani.to_html5_video())\n",
    "#HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
